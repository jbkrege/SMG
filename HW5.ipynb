{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 5 â€” (15 points) - Music structure and song similarity\n",
    "======\n",
    "### What to hand in\n",
    "You are to submit the following things for this homework:\n",
    "1. A Jupyter notebook containing all code and output (figures and audio). I should be able to evaluate the file to reproduce all output. \n",
    "1. Any other data that we tell you to save to a file (e.g. audio files).\n",
    "\n",
    "### How to hand it in\n",
    "To submit your lab:\n",
    "1. Compress all of the files specified into a .zip file. \n",
    "1. Name the file in the following manner, firstname_lastname_hw1.zip. For example, Bryan_Pardo_hw1.zip. \n",
    "1. Submit this .zip file via Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\" class=\"alert alert-danger\"><h3>Due Feb 24 (Friday)</h3></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful reading for this homework - Chapters 4 and 7 in Fundamentals of Music Processing by Meinard Muller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this code block 1st, to import the needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This line is a convenience to import most packages you'll need. You may need to import others (eg random and cmath)\n",
    "import IPython, numpy as np, matplotlib.pyplot as plt, matplotlib, sklearn, librosa, cmath,math, scipy, random\n",
    "from IPython.display import Audio\n",
    "from IPython.display import HTML\n",
    " \n",
    "# This line makes sure your plots happen IN the webpage you're building, instead of in separate windows.\n",
    "%matplotlib inline\n",
    "\n",
    "def apply_style():\n",
    "    \"\"\"\n",
    "\tUseful styles for displaying graphs and audio elements.\n",
    "\t\"\"\"\n",
    "    style = HTML(\"\"\"\n",
    "        <style>\n",
    "            audio {\n",
    "            width: 100% !important;\n",
    "        }\n",
    "        .output_png {\n",
    "            text-align: center !important;\n",
    "        }\n",
    "        </style>\n",
    "        \"\"\")\n",
    "    IPython.display.display(style)\n",
    "    \n",
    "def audio(d, sr, ext = '.mp3'):\n",
    "    \"\"\"\n",
    "\tEmbeds audio into notebook\n",
    "\tParameters:\n",
    "\t   d: numpy array of audio data.\n",
    "\t   sr: sampling rate for the audio\n",
    "\t\"\"\"\n",
    "    IPython.display.display(IPython.display.Audio(data=d, rate = sr))\n",
    "    \n",
    "apply_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>1. (5 points): Self-similarity</b>\n",
    "</div>\n",
    "\n",
    "In this question, you will construct a self-similarity matrix for a song, compare different distance measures used to compute self-similarity, and extract a novelty curve and interpret it.\n",
    "\n",
    "In the perception of music structure, the principles of repetition, novelty, and homogeneity play an important role in defining what makes up a musical \"segment\" (e.g. a chorus). Repetition is the fact that there will be patterns that recur throughout a song (e.g. verse, chorus, hook). Recurrent patterns can be rhythmic, harmonic, or melodic in nature. If a pattern recurs often, like the chorus in a song would, we perceive it as a recurring musical segment. Novelty, on the other hand, is the idea that there will be parts of the song where there is a quick significant change along some perceptual dimension (e.g. going from the verse into the chorus). These changes often mark boundaries between sections of a song. If there is a sharp boundary between the verse and the chorus, we are likely to perceive a difference between the two. Finally, homogeneity is the glue that keeps a segment together - a segment is often characterized by some inherent sameness (e.g. instrumentation, tempo, harmonic material).\n",
    "\n",
    "Repetition, homogeneity, and novelty are fundamental cues for partitioning a song into meaningful musical segments. These three principles can be extracted and visualized by using the *self-similarity matrix*. In this question, we will:\n",
    "1. Compute a self-similarity matrix for a song\n",
    "2. Compare self-similarity when using different features and distance metrics\n",
    "3. Learn how to extract repetition, novelty, and homogeneity from the self-similarity matrix.\n",
    "\n",
    "**Functions you can use**: librosa.stft, librosa.chromagram, librosa.cqt. You may not use anything from librosa.segment (e.g. librosa.segment.recurrence_matrix). The entire numpy and scipy libraries are available for your use (and we highly recommend you look for relevant functions - the functions scipy.spatial.distance.cdist and scipy.spatial.distance.pdist may be useful for this assignment...)\n",
    "\n",
    "Okay, first let's load up some audio - it's Call Me Maybe by Carly Rae Jepsen. Yeah, that's right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "music, sr = librosa.load('music/call_me_maybe.mp3')\n",
    "hop_length = 1024\n",
    "n_fft = 2048\n",
    "stft = librosa.stft(music, hop_length = hop_length, n_fft = n_fft)\n",
    "log_spectrogram = librosa.logamplitude(np.abs(stft**2), ref_power=np.max)\n",
    "audio(music, sr)\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "librosa.display.specshow(log_spectrogram, sr = sr, hop_length = hop_length, y_axis = 'log', x_axis = 'time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualized above is the log spectrogram for the song. We are going to call this something more general for the purposes of this assignment. The log spectrogram is a sequence of *feature vectors*. Features are the same thing as frequency components, spectra, etc. The feature vector in this case are the numbers along each column of the log spectrogram. The sequence part is these features going forward in time. In this case, here's what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'M: %d, N: %d' % log_spectrogram.shape\n",
    "\n",
    "\n",
    "def plot_feature_vector(log_spectrogram, index, hop_length, sr):\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    plt.plot(log_spectrogram[:, int(index)])\n",
    "    plt.title('%dth feature vector' % int(index))\n",
    "    plt.ylabel('dB')\n",
    "    plt.xlabel('Frequency bin')\n",
    "    plt.show()\n",
    "\n",
    "    audio(music[index*hop_length - int(sr*.5):index*hop_length + int(sr*.5)], sr)\n",
    "\n",
    "plot_feature_vector(log_spectrogram, 50, hop_length, sr)\n",
    "plot_feature_vector(log_spectrogram, 55, hop_length, sr)\n",
    "plot_feature_vector(log_spectrogram, log_spectrogram.shape[1]/2, hop_length, sr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "M features in each vector in a sequence of N feature vectors. Feature vectors looks like the plots above. The audio around the feature vector is below each plot. Now consider the feature vectors above. How similar do they sound? What distance measures could you use to compare the feature vectors? List at least three. For each distance measure, define it in mathematical terms and explain how your choice of distance measure relates to your perception of similarity.\n",
    "\n",
    "*(hint: check out https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html for some different distance measures - we recommend at least looking at cosine and euclidean)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER GOES HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, time to make a similarity matrix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine, euclidean, cityblock, cdist\n",
    "\n",
    "def sim_matrix(feature_vectors, sample_rate, hop_length, distance_metric = 'cityblock', display = True):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            feature_vectors - a numpy ndarray MxN, where M is the number of features in each vector and \n",
    "            N is the length of the sequence.\n",
    "            sample_rate - sample rate of the original audio\n",
    "            hop_length - the length of the hop used in the representation\n",
    "            distance_metric - which distance metric to use to compute similarity. Defaults to cosine.\n",
    "            display - whether or not to display the similarity matrix after computing it. Defaults to True.\n",
    "        Output:\n",
    "            if display is True, plot the similarity matrix. Along the x and y axis of the similarity matrix, \n",
    "            the ticks should be in seconds not in samples. \n",
    "            returns sim_matrix - an NxN matrix with the pairwise distance between every feature vector.\n",
    "    \"\"\"\n",
    "    feature_vectors = np.rot90(feature_vectors,3)\n",
    "    distances = cdist(feature_vectors,feature_vectors, distance_metric)\n",
    "    maximum = np.amax(distances)\n",
    "    ret = 1.0-(distances/float(maximum))\n",
    "    if display:\n",
    "        plt.imshow(ret)\n",
    "        skip = feature_vectors.shape[-1] / 10\n",
    "        plt.xticks(np.arange(0, feature_vectors.shape[-1], skip),\n",
    "                   ['%.2f' % (i * hop_length / float(sample_rate)) for i in range(feature_vectors.shape[-1])][::skip],\n",
    "                   rotation='vertical')\n",
    "        plt.yticks(np.arange(0, feature_vectors.shape[-1], skip),\n",
    "                   ['%.2f' % (i * hop_length / float(sample_rate)) for i in range(feature_vectors.shape[-1])][::skip])\n",
    "        plt.xlabel('Time (s)')\n",
    "        plt.ylabel('Time (s)')\n",
    "        plt.title('Similarity matrix')\n",
    "        plt.show()\n",
    "    return ret\n",
    "#     ret = np.zeros(shape = (feature_vectors.shape,feature_vectors.shape))\n",
    "#     for x_i in xrange(feature_vectors):\n",
    "#         x = feature_vectors[x_i]\n",
    "#         for y_i in xrange(feature_vectors):\n",
    "#             y = feature_vectors[y_i]\n",
    "#             if distance_metric == 'cosine':\n",
    "#                 ret[x_i][y_i] = cosine(x,y)\n",
    "#             elif distance_metric == 'euclidean':\n",
    "#                 ret[x_i][y_i] = euclidean(x,y)\n",
    "#             else: #distance_metric == Manhattan\n",
    "#                 ret[x_i][y_i] = cityblock(x,y)\n",
    "#     maximum = np.nanmax(ret)\n",
    "#     ret = 1 - (ret/maximum)\n",
    "#     print \"DONE!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each distance measure you defined above, compute and display the self-similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Idk whats up with the labels... theyre the ones posted on piazza...\"\n",
    "feature_vectors = log_spectrogram\n",
    "matrix = sim_matrix(feature_vectors, sr, hop_length, \"euclidean\")\n",
    "feature_vectors = log_spectrogram\n",
    "matrix1 = sim_matrix(feature_vectors, sr, hop_length, \"cosine\")\n",
    "feature_vectors = log_spectrogram\n",
    "matrix2 = sim_matrix(feature_vectors, sr, hop_length, \"cityblock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting the similarity matrix**\n",
    "\n",
    "**1. What does the main long diagonal going across each similarity matrix from top left to bottom right mean?**\n",
    "The dark line is all 1's. This is because the distance between any frame to itself is zero.  \n",
    "     \n",
    "**2. How can the blocky structures in the similarity matrix be interpreted? **     \n",
    "The blocks are sections of the song. The verse/prechorus appears as a layer on the top and left side of the chorus, the densely orange block that appears three times during the song (observed by tracing the diagonal line).\n",
    "     \n",
    "**3. How are repeating patterns encoded in the similarity matrix?**        \n",
    "Repetition of a section is apparent by looking across the row/down the column and seeing where there are colors with higher energy with a similar dimension to the original section. For example, the pizz intro is represented by a orange box that repeats three times in the song - shown by the three orange boxes along the left-most side of the matrix.\n",
    "     \n",
    "**4. How can edge structures in the similarity matrix be interpreted?**  \n",
    "The beginning and end of the track is silence, which is depicted as dark red in the four corners because silence is always identical to itself. Since the rest of the track has very little silence, the rest of the boarder is blue.              \n",
    "\n",
    "**5. How did your distance measure affect your ability to find repetition, homogeneity, and novelty in the similarity matrix?**     \n",
    "Euclidean and cityblock distance measures had visually similar results, and are both helpful in identifying repetition and novelty because sections are clearly defined. Cityblock seemed to have more contrast so I will use it for the duration of this assignment. The cosine matrix, which is more sensitive to changes in volume, was almost all red showing that there is very little dynamic contrast - in other words the song is dynamically homogenious. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going forward, based on your answer to the 5th part of the question above, pick one distance measure to use for the rest of this part of the assignment.\n",
    "\n",
    "**Using different features**\n",
    "\n",
    "Let's compute a similarity matrix from a different feature vector. Use [librosa.feature.mfcc](https://librosa.github.io/librosa/generated/librosa.feature.mfcc.html) and [librosa.feature.chroma_stft](https://librosa.github.io/librosa/generated/librosa.feature.chroma_stft.html). Compute and display similarity matrices for both of these with your chosen distance measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chroma_features = librosa.feature.chroma_stft(music, sr)\n",
    "cepstrum_features = librosa.feature.mfcc(music, sr)\n",
    "print \"Chroma Similarity\"\n",
    "matrix = sim_matrix(chroma_features, sr, hop_length, \"cityblock\")\n",
    "print \"Cepstrum Similarity\"\n",
    "matrix1 = sim_matrix(cepstrum_features, sr, hop_length, \"cityblock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. How do your choice of features affect the similarity matrix?**\n",
    "The chroma similarity matrix looked much more random than those calculated previously using the log spectrogram. This is suprising to me because I would expect it to be simmilar to the log spectrogram becasue the spectrogram is also using pitch. The cepstrum looks like the log spectrogram matrix, but with more green, indicating that there is more novelty in the tambre than pitches. \n",
    "\n",
    "**2. Looking at the similarity matrix, what is similarity in this specific song dominated by?**     \n",
    "Similarity in this song is clearly dominated by tambre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>2. (5 points): Building an infinite jukebox using self-similarity with beat synchronous features</b>\n",
    "</div>\n",
    "\n",
    "In this part, we will learn how to extract beat synchronous features from a song. These features will be plugged into a similarity matrix to create a version of the [Infinite Jukebox](http://labs.echonest.com/Uploader/index.html?trid=TRORQWV13762CDDF4C).\n",
    "\n",
    "In the previous part, we created self-similarity matrices from the STFT, the MFCC-gram, and the Chromagram. One issue with each of these is that the similarity measure is done at the level of the frame, rather than at the level of a meaningful musical structure. In this part, we instead consider self-similarity at the \"beat\" level, by using the beat tracker from librosa.\n",
    "\n",
    "To make features beat synchronous, there are a couple things you should do:\n",
    "\n",
    "1. Get the features (let's use librosa.feature.chroma_stft to get them).\n",
    "2. Get the beats (let's use librosa.beat.beat_track for that).\n",
    "3. For every feature vector between beat boundaries, you need a way to aggregate them into a single column. We'll examine the effect of different aggregators for beat synchronous features below. The idea is to take the chunk of feature vectors within a beat and aggregate them into a single feature vector using something like np.median, np.mean, np.max, np.sum.\n",
    "\n",
    "Write code to do this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def beat_track(music, sr, hop_length):\n",
    "    \"\"\"\n",
    "        input: \n",
    "            music: an audio signal, single channel array containing all the samples of the signal.\n",
    "            sr: sample rate to use\n",
    "            hop_length: the hop length for the stft - without specifying this, the frames output by the beat tracker \n",
    "            may not match your feature vector sequence.\n",
    "        output:\n",
    "            beats: a list of all the frame indices found by the beat tracker\n",
    "    \"\"\"\n",
    "    tempo,beats = librosa.beat.beat_track(music, sr, start_bpm = 120, hop_length = hop_length)\n",
    "    return beats\n",
    "\n",
    "def beat_sync_features(feature_vectors, beats, aggregator = np.median, display = False):\n",
    "    \"\"\"\n",
    "        input:\n",
    "            feature_vectors: a numpy ndarray MxN, where M is the number of features in each vector and \n",
    "            N is the length of the sequence.\n",
    "            beats: frames given by the beat tracker\n",
    "            aggregator: how to summarize all the frames within a beat (e.g. np.median, np.mean). Defaults to np.median.\n",
    "            display: if True, displays the beat synchronous features.\n",
    "        output:\n",
    "            beat_synced_features: a numpy ndarray MxB, where M is the number of features in each vector\n",
    "            and B is the number of beats. Each column of this matrix represents a beat synchronous feature\n",
    "            vector.\n",
    "    \"\"\"\n",
    "    nBeats = beats.shape[0]\n",
    "    nVectors = feature_vectors.shape[0]\n",
    "    start = 0\n",
    "    ret = np.zeros((nBeats,nVectors))\n",
    "    for b in xrange(nBeats):\n",
    "        ret[b] = aggregator(feature_vectors[:,start:beats[b]],axis=1)\n",
    "        start = beats[b]\n",
    "#     ret = np.array(ret)\n",
    "    ret = np.rot90(ret)\n",
    "    if display:\n",
    "        x = np.linspace(0,nBeats,nBeats)\n",
    "        y = np.linspace(0,nVectors,nVectors)\n",
    "        plt.pcolormesh(x, y, ret)\n",
    "        plt.show()\n",
    "    return np.nan_to_num(ret)\n",
    "#     nBeats = beats.shape[0]\n",
    "#     nVectors = feature_vectors.shape[0]\n",
    "#     feature_vectors = np.rot90(feature_vectors,3)\n",
    "#     ret = []\n",
    "#     for i in xrange(nBeats-1):\n",
    "#         start = beats[i]    \n",
    "#         end = beats[i+1]\n",
    "#         beat = feature_vectors[start:end,0:]\n",
    "#         ret.append(aggregator(beat,0))\n",
    "#     ret = np.array(ret)\n",
    "#     ret = np.rot90(ret,1)\n",
    "#     if display:\n",
    "#         y = np.linspace(0,feature_vectors.shape[1],feature_vectors.shape[1])\n",
    "#         x = np.linspace(0,nBeats-1,nBeats-1)\n",
    "#         plt.pcolormesh(x ,y ,ret)\n",
    "#         plt.show()\n",
    "#     return np.nan_to_num(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, extract chroma features from the music. Then beat track the music and create and display beat synchronous feature vector sequences using three different aggregators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "music, sr = librosa.load('music/call_me_maybe.mp3')\n",
    "hop_length = 1024\n",
    "n_fft = 2048\n",
    "\n",
    "beats = beat_track(music,sr,hop_length)\n",
    "# print beats\n",
    "print \"num beats\",beats.shape\n",
    "feature_vectors = librosa.stft(music, hop_length = hop_length, n_fft = n_fft)\n",
    "print \"f_vectors shape\", feature_vectors.shape\n",
    "beat_synced_features = beat_sync_features(feature_vectors, beats)\n",
    "matrix = sim_matrix(beat_synced_features, sr, hop_length)\n",
    "print \"Idk whats up with the spacing but it works better with the chroma later...You'll see...\"\n",
    "beat_synced_features = beat_sync_features(feature_vectors, beats, np.sum)\n",
    "matrix = sim_matrix(beat_synced_features, sr, hop_length)\n",
    "beat_synced_features = beat_sync_features(feature_vectors, beats, np.mean)\n",
    "matrix = sim_matrix(beat_synced_features, sr, hop_length)\n",
    "# print aggregator(test,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on your results above, pick one of the aggregators and justify its use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median seems to detect simmilarity the best becasue it picks up more simmilarities that aren't in the blocks (becasue it isn't influenced by outliers), so I'll go with that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, show the similarity matrix for your beat synchronous features with your chosen aggregator and chosen distance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bsf = beat_sync_features(feature_vectors, beats)\n",
    "matrix = sim_matrix(bsf, sr, hop_length)\n",
    "print \"Ok, I know this looks weird, but I think its just in the graph... It seems to function correctly.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're ready to put together an infinite jukebox! The infinite jukebox works by using the beat-synchronous self-similarity matrix to hop to different beats that are very similar to the upcoming beat with some probability. Given a good similarity measure and a good beat tracker, when the song jumps from beat to beat you shouldn't really notice it happening! Here's the pseudocode:\n",
    "\n",
    "1. Extract the regular features (not beat synchronous).\n",
    "2. Extract the beats using the beat tracker.\n",
    "3. Aggregate the features into beat synchronous features.\n",
    "4. Compute the beat synchronous self-similarity matrix.\n",
    "5. Initialize the output of the jukebox.\n",
    "5. Starting at the 1st beat, with some probability either pick the next beat in the song and tack it onto the jukebox output OR pick something very similar to the next beat in the song using the beat synchronous self-similarity matrix. This other beat could be anywhere in the song. Make sure not to jump to the beat you're on or you'll get stuck in a loop!\n",
    "6. Repeat until your computer dies (or until you've exceeded the desired length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_music_samples(music, hop_length, start_frame, end_frame):\n",
    "    \"\"\"\n",
    "        input:\n",
    "            music - vector of samples for the musical signal - output by librosa.\n",
    "            hop_length - hop_length for the feature vector computation\n",
    "            start_frame - frame index to start at\n",
    "            end_frame - frame index to end at\n",
    "        output:\n",
    "            chunk - samples corresponding to between and including the start and end frame\n",
    "    \"\"\"\n",
    "    chunk = music[start_frame * hop_length: end_frame * hop_length]\n",
    "    return chunk\n",
    "    \n",
    "\n",
    "def compute_beat_sync_chroma(music, sr, hop_length):\n",
    "    \"\"\"\n",
    "        input:\n",
    "            music - vector of samples for the musical signal - output by librosa.\n",
    "            sr - sample rate for the audio\n",
    "            hop_length - hop_length for the feature vector computation\n",
    "        output: \n",
    "            beat_synced_features - beat synchronous chroma features for the signal\n",
    "    \"\"\"\n",
    "    beats = beat_track(music, sr, hop_length)\n",
    "    chroma_features = librosa.feature.chroma_stft(music, sr)\n",
    "    return beat_sync_features(chroma_features, beats, aggregator = np.median, display = False)\n",
    "    \n",
    "    \n",
    "def compute_beat_sync_cepstra(music, sr, hop_length):\n",
    "    \"\"\"\n",
    "        input:\n",
    "            music - vector of samples for the musical signal - output by librosa.\n",
    "            sr - sample rate for the audio\n",
    "            hop_length - hop_length for the feature vector computation\n",
    "        output: \n",
    "            beat_synced_features - beat synchronous cepstra features for the signal\n",
    "    \"\"\"\n",
    "    beats = beat_track(music, sr, hop_length)\n",
    "    cepstrum_features = librosa.feature.mfcc(music, sr)\n",
    "    return beat_sync_features(cepstrum_features, beats, aggregator = np.median, display = False)\n",
    "    \n",
    "    \n",
    "def infinite_jukebox(music, sr,  hop_length, branching_probability, threshold, output_length, features = \"chroma\"):\n",
    "    \"\"\"\n",
    "        input:\n",
    "            music - vector of samples for the musical signal - output by librosa.\n",
    "            sr - sample rate for the audio\n",
    "            hop_length - hop_length for the feature vector computation\n",
    "            branching_probability - how likely are we to pick the next beat? If its high, we never pick the next beat.\n",
    "            If it is 0, we just end up getting the song back in the same order.\n",
    "            threshold - what is the threshold of similarity for jumping to a new beat?\n",
    "            output_length - how long do you want the output of your infinite jukebox to be? It unfortunately cannot\n",
    "            actually be infinite. :(\n",
    "        output:\n",
    "            jukebox_output - vector of samples of length output_length * sr containing the results of your code.\n",
    "    \"\"\"\n",
    "    sample_length = music.shape[0]/sr #length in seconds\n",
    "    beats = beat_track(music, sr, hop_length)\n",
    "    bps = beats.shape[0]/sample_length\n",
    "    nOutputBeats = int(output_length*bps*2)\n",
    "    if features == 'chroma':\n",
    "        features = librosa.feature.chroma_stft(music, sr)\n",
    "    elif features == 'cepstra':\n",
    "        features = librosa.feature.mfcc(music, sr)\n",
    "    else:\n",
    "        raise ValueError(\"oops we don't have that feature yet\")\n",
    "    bsf = beat_sync_features(features, beats, aggregator = np.median, display = False)\n",
    "    matrix = sim_matrix(bsf, sr, hop_length)\n",
    "    ret = np.array([0])\n",
    "    ret = np.append(ret,get_music_samples(music, hop_length, beats[0], beats[1]))\n",
    "    curri = 1 #Current beat index\n",
    "    for i in xrange(nOutputBeats-1):\n",
    "        rando = random.random()\n",
    "        if (rando < branching_probability) and (curri < beats.shape[0]-1):#Don't skip.\n",
    "            ret = np.append(ret,get_music_samples(music, hop_length, beats[curri], beats[curri+1]))\n",
    "            curri += 1\n",
    "        else:#skip\n",
    "            randomBeats = random.sample(range(beats.shape[0]), beats.shape[0]) #array of random beat indexs\n",
    "            #find another similar frame\n",
    "            matchFound = False\n",
    "            for j in xrange(beats.shape[0]-1):\n",
    "                ranBeat = randomBeats[j]\n",
    "                if matrix[curri][ranBeat] > threshold:\n",
    "                    if (ranBeat < beats.shape[0]-1):#end case\n",
    "                        print \"Jump \",ranBeat\n",
    "                        ret = np.append(ret,get_music_samples(music, hop_length, beats[ranBeat], beats[ranBeat+1]))\n",
    "                        curri = ranBeat+1\n",
    "                        matchFound = True\n",
    "                        break\n",
    "            if not(matchFound):#just add the next beat\n",
    "                print \"no match found\"\n",
    "                ret = np.append(ret,get_music_samples(music, hop_length, beats[curri], beats[curri+1]))\n",
    "                curri += 1\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, run your jukebox! Play with the similarity threshold and the branching probability. Show us some variants. Keep the examples somewhat short (like 2 minutes tops). Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"A quite nice juke\"\n",
    "juke = infinite_jukebox(music, sr,  hop_length, .9, .97, 30)\n",
    "audio(juke, sr)\n",
    "\"A very skippy one\"\n",
    "juke = infinite_jukebox(music, sr,  hop_length, .3, .96, 30)\n",
    "audio(juke, sr)\n",
    "\"Gettin a little wild\"\n",
    "juke = infinite_jukebox(music, sr,  hop_length, .2, .7, 30)\n",
    "audio(juke, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>3. (5 points): Cross similarity: comparing songs using beat synchronous features</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've looked now at self-similarity within a single song. We can extend these concepts to define song similarity - the similarity between two different songs. We can then use the similarity measure for cover song identification. Let's look at the following cover of Call Me Maybe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cover, sr = librosa.load('music/call_me_maybe_cover.mp3', offset = 40, duration = 190)\n",
    "hop_length = 1024\n",
    "n_fft = 2048\n",
    "stft = librosa.stft(cover, hop_length = hop_length, n_fft = n_fft)\n",
    "log_spectrogram = librosa.logamplitude(np.abs(stft**2), ref_power=np.max)\n",
    "audio(cover, sr)\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "librosa.display.specshow(log_spectrogram, sr = sr, hop_length = hop_length, y_axis = 'log', x_axis = 'time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the space below, list all of the things you notice that are different between the cover and the original. What changed? When doing song similarity, we want to be robust to these changes. What changes will the beat synchronous chroma features we've been using so far be robust to, and what changes will it not be robust to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Different instrumentation. Piano+guitar intro. Cheezy e bass in the chorus. Distorted guitar later. Arpegiating guitar in background.     \n",
    "2) V different voice. Angsty alternative boy band voice instead of babe pop star     \n",
    "3) Singer uses more uhh stylistic liberties      \n",
    "\n",
    "The chroma features will be robust to the changes in tambre, but we cannot avoid the added notes in the apeggiating guitar and the stylistic liberties. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, alter your earlier sim_matrix function to become a cross_sim_matrix function. cross_sim_matrix computes cross-similarity between two sets of feature vectors rather than self-similarity within a single feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## YOUR ANSWER GOES HERE\n",
    "\n",
    "def cross_sim_matrix(feature_vectors_a, feature_vectors_b, sample_rate, hop_length, distance_metric = 'cosine', display = True):\n",
    "    \"\"\"\n",
    "        Input:\n",
    "            feature_vectors_a - a numpy ndarray MxN, where M is the number of features in each vector and \n",
    "            N is the length of the sequence. Corresponds to the reference song.\n",
    "            feature_vectors_b - a numpy ndarray MxN, where M is the number of features in each vector and \n",
    "            N is the length of the sequence. Corresponds to the cover song.\n",
    "            sample_rate - sample rate of the original audio\n",
    "            hop_length - how many samples are in each frame\n",
    "            distance_metric - which distance metric to use to compute similarity. Defaults to cosine.\n",
    "            display - whether or not to display the similarity matrix after computing it. Defaults to True.\n",
    "        Output:\n",
    "            if display is True, plot the similarity matrix. Along the x and y axis of the similarity matrix, \n",
    "            the ticks should be in seconds not in samples. \n",
    "            returns cross_sim_matrix - an NxN matrix with the pairwise distance between every feature vector.\n",
    "    \"\"\"\n",
    "    feature_vectors_a = np.rot90(feature_vectors_a,3)\n",
    "    feature_vectors_b = np.rot90(feature_vectors_b,3)\n",
    "    distances = cdist(feature_vectors_a,feature_vectors_b, distance_metric)\n",
    "    maximum = np.amax(distances)\n",
    "    ret = 1.0-(distances/float(maximum))\n",
    "    if display:\n",
    "        plt.imshow(ret)\n",
    "        skip = feature_vectors.shape[-1] / 10\n",
    "        plt.xticks(np.arange(0, feature_vectors_a.shape[-1], skip),\n",
    "                   ['%.2f' % (i * hop_length / float(sample_rate)) for i in range(feature_vectors_a.shape[-1])][::skip],\n",
    "                   rotation='vertical')\n",
    "        plt.yticks(np.arange(0, feature_vectors_b.shape[-1], skip),\n",
    "                   ['%.2f' % (i * hop_length / float(sample_rate)) for i in range(feature_vectors_b.shape[-1])][::skip])\n",
    "        plt.xlabel(\"Original\")\n",
    "        plt.ylabel(\"Cover\")\n",
    "        plt.title('Similarity matrix')\n",
    "        plt.show()\n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the beat synchronous features for both the cover and the reference, and their cross similarity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "song_chroma = compute_beat_sync_chroma(music, sr, hop_length)\n",
    "cover_chroma = compute_beat_sync_chroma(cover, sr, hop_length)\n",
    "cmatrix = cross_sim_matrix(song_chroma, cover_chroma, sr, hop_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How'd it look? Not very similar, I bet. Why? Well looking at the beat synchronous chroma features, it should be clear that the two songs are in different keys. When you try to do the distance computation on each chroma feature vector, it will fail because the energy is in different bins! Now we'll write code to fix this and compute a proper cross-similarity measure. \n",
    "\n",
    "We can fix this by transposing one of the chromagrams so that the two songs are compared within the same key. Below, write some code to tranpose a chromagram by an arbitrary number of half steps. Rows of the chromagram should shift up or down by however many half steps. Rows at the top will wrap around to the bottom.\n",
    "\n",
    "*Hint: np.roll is useful!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## YOUR ANSWER GOES HERE\n",
    "\n",
    "def transpose_chromagram(chroma, n_steps, display = True):\n",
    "    \"\"\"\n",
    "        input: \n",
    "            chroma - 12xN numpy array. \n",
    "            n_steps - how many half steps to transpose by\n",
    "            display - show the transposed chromagram\n",
    "        output: a transposed version of the chroma\n",
    "    \"\"\"\n",
    "    transposed_chroma = np.roll((chroma),n_steps, axis = 0)\n",
    "    if display:\n",
    "        plt.figure(figsize=(20, 12))\n",
    "        plt.imshow(transposed_chroma, aspect = 3)\n",
    "        plt.show()\n",
    "    return transposed_chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code below to find the optimal transposition (the actual number of half steps between the cover and the song) by looking at the cross-similarity matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for n_step in range(12):  \n",
    "    print n_step\n",
    "    tp_cover_chroma = transpose_chromagram(cover_chroma, n_step)\n",
    "    song_chroma = transpose_chromagram(song_chroma, 0)\n",
    "    csm = cross_sim_matrix(song_chroma, tp_cover_chroma, sr, hop_length, distance_metric = 'euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've got the optimal transposition index, run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_step = 9\n",
    "tp_cover_chroma = transpose_chromagram(cover_chroma, n_step)\n",
    "song_chroma = transpose_chromagram(song_chroma, 0)\n",
    "csm = cross_sim_matrix(song_chroma, tp_cover_chroma, sr, hop_length, distance_metric = 'correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switching the distance metric from euclidean to correlation distance is helpful - the two songs have different loudness characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's make a version of the Infinite Jukebox that jumps between the song and its cover! What it should do is use the cross-similarity matrix to hop between the song and the cover. Everything else is the same as the regular infinite jukebox, but you'll want to toggle between jumping from song to cover and cover to song, depending on where you are. Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## YOUR ANSWER GOES HERE\n",
    "    \n",
    "def cover_infinite_jukebox(song, cover, sr, hop_length, branching_probability, threshold, output_length, transposition = 9):\n",
    "    \"\"\"\n",
    "        input:\n",
    "            song - vector of samples for the original song - output by librosa.\n",
    "            cover - vector of samples for the cover song - output by librosa\n",
    "            sr - sample rate for the audio\n",
    "            hop_length - hop_length for the feature vector computation\n",
    "            branching_probability - how likely are we to pick the next beat? If its high, we never pick the next beat.\n",
    "            If it is 0, we just end up getting the song back in the same order.\n",
    "            threshold - what is the threshold of similarity for jumping to a new beat?\n",
    "            output_length - how long do you want the output of your infinite jukebox to be? It unfortunately cannot\n",
    "            actually be infinite. :(\n",
    "        output:\n",
    "            jukebox_output - vector of samples of length output_length * sr containing the results of your code.\n",
    "    \"\"\"\n",
    "    song_chroma = compute_beat_sync_chroma(song, sr, hop_length)\n",
    "    cover_chroma = compute_beat_sync_chroma(cover, sr, hop_length)\n",
    "    tp_cover_chroma = transpose_chromagram(cover_chroma, transposition, display = False)\n",
    "    song_chroma = transpose_chromagram(song_chroma, 0, display = False)\n",
    "    csm = cross_sim_matrix(song_chroma, tp_cover_chroma, sr, hop_length, distance_metric = 'correlation')\n",
    "    print 'csm', csm.shape\n",
    "    #Calculate number of output beats\n",
    "    sample_length = song.shape[0]/sr #length in seconds\n",
    "    beats = beat_track(song, sr, hop_length)\n",
    "    bps = beats.shape[0]/sample_length\n",
    "    nOutputBeats = int(output_length*bps*2)\n",
    "    \n",
    "#     if features == 'chroma':\n",
    "#         features = librosa.feature.chroma_stft(music, sr)\n",
    "#     elif features == 'cepstra':\n",
    "#         features = librosa.feature.mfcc(music, sr)\n",
    "#     else:\n",
    "#         raise ValueError(\"oops we don't have that feature yet\")\n",
    "#     bsf = beat_sync_features(features, beats, aggregator = np.median, display = False)\n",
    "#     matrix = sim_matrix(bsf, sr, hop_length)\n",
    "    music = song\n",
    "    musicBool = 0\n",
    "    ret = np.array([0])\n",
    "    ret = np.append(ret,get_music_samples(music, hop_length, beats[0], beats[1]))\n",
    "    curri = 1 #Current beat index\n",
    "    for i in xrange(nOutputBeats-1):\n",
    "#***Code for ***\n",
    "#         #Branch to other?\n",
    "#         rando = random.random()\n",
    "#         if (rando < branching_probability) and (curri < beats.shape[0]-1):\n",
    "#             print \"Switch!\"\n",
    "#             if musicBool:\n",
    "#                 music = cover\n",
    "#                 musicBool = 0\n",
    "#             else:\n",
    "#                 music = song\n",
    "#                 musicBool = 1\n",
    "#         #Branch to other place?\n",
    "        rando = random.random()\n",
    "        if (rando > branching_probability) and (curri < beats.shape[0]-1):#Don't skip.\n",
    "            ret = np.append(ret,get_music_samples(music, hop_length, beats[curri], beats[curri+1]))\n",
    "            curri += 1\n",
    "        else:#skip\n",
    "            randomBeats = random.sample(range(beats.shape[0]), beats.shape[0]) #array of random beat indexs\n",
    "            #find another similar frame\n",
    "            matchFound = False\n",
    "            if (musicBool == 0):\n",
    "                music = song\n",
    "                musicBool = 1\n",
    "            else:\n",
    "                music = cover\n",
    "                musicBool = 0\n",
    "            for j in xrange(beats.shape[0]-1):\n",
    "                ranBeat = randomBeats[j]\n",
    "                if musicBool:\n",
    "                    x = curri\n",
    "                    y = ranBeat\n",
    "                else:\n",
    "                    x = ranBeat\n",
    "                    y = curri\n",
    "#                 if ((x < csm.shape[0]) and (y < csm.shape[1])):\n",
    "#                     print 'csm',csm[x][y]\n",
    "                if (x < csm.shape[0]) and (y < csm.shape[1]) and (csm[x][y] > threshold) and (curri != ranBeat):\n",
    "                    print \"Jump \",ranBeat\n",
    "                    ret = np.append(ret,get_music_samples(music, hop_length, beats[ranBeat], beats[ranBeat+1]))\n",
    "                    curri = ranBeat+1\n",
    "                    matchFound = True\n",
    "                    break\n",
    "            if not(matchFound):#just add the next beat\n",
    "                print \"no match found\"\n",
    "                ret = np.append(ret,get_music_samples(music, hop_length, beats[curri], beats[curri+1]))\n",
    "                curri += 1\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, show a couple variants of the cover_infinite_jukebox with different branching probabilities and threshold, like you did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "song, sr = librosa.load('music/call_me_maybe.mp3')\n",
    "cover, sr = librosa.load('music/call_me_maybe_cover.mp3', offset = 40, duration = 190)\n",
    "\n",
    "\"A quite nice juke\"\n",
    "juke = cover_infinite_jukebox(song, cover, sr,  hop_length, .1, .9, 30)\n",
    "audio(juke, sr)\n",
    "\"A very skippy one\"\n",
    "juke = cover_infinite_jukebox(song, cover, sr,  hop_length, .5, .9, 30)\n",
    "audio(juke, sr)\n",
    "\"Gettin a little wild\"\n",
    "juke = cover_infinite_jukebox(song, cover, sr,  hop_length, .9, .7, 30)\n",
    "audio(juke, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we're done! Zip it up and turn it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
